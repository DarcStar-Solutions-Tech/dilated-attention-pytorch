# Ring Attention V2 Benchmark Results

**Date**: 2025-06-27-1933-UTC  
**Git Commit**: 56bfe37e  
**Hardware**: NVIDIA GeForce GTX 1080  

## Parameters

- **implementations**: ['StandardAttention', 'RingAttentionCorrectV2', 'RingDilatedAttentionV2']
- **description**: Comprehensive benchmark of corrected Ring Attention implementations

## Detailed Results

| Implementation | RingAttentionCorrectV2 | RingDilatedAttentionV2 | StandardAttention | best_memory_reduction_percent | implementations_tested | max_sequence_tested |
| --- | --- | --- | --- | --- | --- | --- |
| benchmark_results | {'implementation': 'RingAttentionCorrectV2', 'device': 'cuda', 'dtype': 'torch.float16', 'seq_length_results': {1024: {1: {'avg_time_ms': 7.790207862854004, 'std_time_ms': 2.1642446517944336, 'throughput_tokens_per_sec': 131447.07022295674, 'runs': 2, 'peak_memory_mb': 49.0625, 'theoretical_memory_mb': 4.0, 'memory_efficiency': 0.08152866242038216}, 2: {'avg_time_ms': 5.763649940490723, 'std_time_ms': 0.853419303894043, 'throughput_tokens_per_sec': 177665.19663281558, 'runs': 2, 'peak_memory_mb': 25.0625, 'theoretical_memory_mb': 3.0, 'memory_efficiency': 0.11970074812967581}, 4: {'avg_time_ms': 7.061481475830078, 'std_time_ms': 0.5061626434326172, 'throughput_tokens_per_sec': 145012.0634749139, 'runs': 2, 'peak_memory_mb': 13.0625, 'theoretical_memory_mb': 2.5, 'memory_efficiency': 0.19138755980861244}, 8: {'avg_time_ms': 6.622910499572754, 'std_time_ms': 0.20182132720947266, 'throughput_tokens_per_sec': 154614.80267113054, 'runs': 2, 'peak_memory_mb': 7.0625, 'theoretical_memory_mb': 2.25, 'memory_efficiency': 0.3185840707964602}, 16: {'avg_time_ms': 11.927127838134766, 'std_time_ms': 0.22864341735839844, 'throughput_tokens_per_sec': 85854.70147523288, 'runs': 2, 'peak_memory_mb': 5.046875, 'theoretical_memory_mb': 2.125, 'memory_efficiency': 0.42105263157894735}}, 2048: {1: {'avg_time_ms': 21.664738655090332, 'std_time_ms': 1.782536506652832, 'throughput_tokens_per_sec': 94531.48882175892, 'runs': 2, 'peak_memory_mb': 194.125, 'theoretical_memory_mb': 8.0, 'memory_efficiency': 0.0412105602060528}, 2: {'avg_time_ms': 96.11630439758301, 'std_time_ms': 50.22406578063965, 'throughput_tokens_per_sec': 21307.519185595116, 'runs': 2, 'peak_memory_mb': 98.125, 'theoretical_memory_mb': 6.0, 'memory_efficiency': 0.061146496815286625}, 4: {'avg_time_ms': 73.64344596862793, 'std_time_ms': 17.85874366760254, 'throughput_tokens_per_sec': 27809.67094984185, 'runs': 2, 'peak_memory_mb': 50.125, 'theoretical_memory_mb': 5.0, 'memory_efficiency': 0.09975062344139651}, 8: {'avg_time_ms': 30.007600784301758, 'std_time_ms': 5.75566291809082, 'throughput_tokens_per_sec': 68249.37504071952, 'runs': 2, 'peak_memory_mb': 26.125, 'theoretical_memory_mb': 4.5, 'memory_efficiency': 0.1722488038277512}, 16: {'avg_time_ms': 27.49943733215332, 'std_time_ms': 1.800537109375, 'throughput_tokens_per_sec': 74474.25106423562, 'runs': 2, 'peak_memory_mb': 14.125, 'theoretical_memory_mb': 4.25, 'memory_efficiency': 0.3008849557522124}}, 4096: {1: {'avg_time_ms': 127.38335132598877, 'std_time_ms': 20.497441291809082, 'throughput_tokens_per_sec': 32154.90845046038, 'runs': 2, 'peak_memory_mb': 772.25, 'theoretical_memory_mb': 16.0, 'memory_efficiency': 0.020718679184202008}, 2: {'avg_time_ms': 212.2400999069214, 'std_time_ms': 83.609938621521, 'throughput_tokens_per_sec': 19298.89781335532, 'runs': 2, 'peak_memory_mb': 388.25, 'theoretical_memory_mb': 12.0, 'memory_efficiency': 0.030907920154539602}, 4: {'avg_time_ms': 111.69421672821045, 'std_time_ms': 2.254605293273926, 'throughput_tokens_per_sec': 36671.549521377136, 'runs': 2, 'peak_memory_mb': 196.25, 'theoretical_memory_mb': 10.0, 'memory_efficiency': 0.050955414012738856}, 8: {'avg_time_ms': 588.9191627502441, 'std_time_ms': 314.3482208251953, 'throughput_tokens_per_sec': 6955.114146518408, 'runs': 2, 'peak_memory_mb': 100.25, 'theoretical_memory_mb': 9.0, 'memory_efficiency': 0.08977556109725686}, 16: {'avg_time_ms': 150.10130405426025, 'std_time_ms': 12.170672416687012, 'throughput_tokens_per_sec': 27288.23727287095, 'runs': 2, 'peak_memory_mb': 52.25, 'theoretical_memory_mb': 8.5, 'memory_efficiency': 0.16267942583732056}}, 8192: {1: {'error': 'CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 7.88 GiB of which 1.05 GiB is free. Process 3093175 has 16.45 MiB memory in use. Including non-PyTorch memory, this process has 2.18 GiB memory in use. Of the allocated memory 2.04 GiB is allocated by PyTorch, and 21.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)'}, 2: {'avg_time_ms': 711.7308378219604, 'std_time_ms': 284.490704536438, 'throughput_tokens_per_sec': 11509.969169059988, 'runs': 2, 'peak_memory_mb': 1544.5, 'theoretical_memory_mb': 24.0, 'memory_efficiency': 0.015539009388151505}, 4: {'avg_time_ms': 907.1938991546631, 'std_time_ms': 522.9716300964355, 'throughput_tokens_per_sec': 9030.043089612296, 'runs': 2, 'peak_memory_mb': 776.5, 'theoretical_memory_mb': 20.0, 'memory_efficiency': 0.025756600128783}, 8: {'avg_time_ms': 809.5630407333374, 'std_time_ms': 316.13290309906006, 'throughput_tokens_per_sec': 10119.039022062236, 'runs': 2, 'peak_memory_mb': 392.5, 'theoretical_memory_mb': 18.0, 'memory_efficiency': 0.045859872611464965}, 16: {'avg_time_ms': 1224.604606628418, 'std_time_ms': 657.1404933929443, 'throughput_tokens_per_sec': 6689.506111327001, 'runs': 2, 'peak_memory_mb': 200.5, 'theoretical_memory_mb': 17.0, 'memory_efficiency': 0.08478802992518704}}}} | {'implementation': 'RingDilatedAttentionV2', 'device': 'cuda', 'dtype': 'torch.float16', 'seq_length_results': {1024: {1: {'avg_time_ms': 1.2720823287963867, 'std_time_ms': 0.004172325134277344, 'throughput_tokens_per_sec': 804979.3451410364, 'runs': 2, 'peak_memory_mb': 33.0, 'theoretical_memory_mb': 4.0, 'memory_efficiency': 0.12121212121212122}, 2: {'avg_time_ms': 5.383610725402832, 'std_time_ms': 0.3255605697631836, 'throughput_tokens_per_sec': 190206.9172958969, 'runs': 2, 'peak_memory_mb': 25.0625, 'theoretical_memory_mb': 3.0, 'memory_efficiency': 0.11970074812967581}, 4: {'avg_time_ms': 6.818175315856934, 'std_time_ms': 0.7168054580688477, 'throughput_tokens_per_sec': 150186.80989596993, 'runs': 2, 'peak_memory_mb': 13.0625, 'theoretical_memory_mb': 2.5, 'memory_efficiency': 0.19138755980861244}, 8: {'avg_time_ms': 7.039427757263184, 'std_time_ms': 0.6848573684692383, 'throughput_tokens_per_sec': 145466.36961270767, 'runs': 2, 'peak_memory_mb': 7.0625, 'theoretical_memory_mb': 2.25, 'memory_efficiency': 0.3185840707964602}, 16: {'avg_time_ms': 14.444708824157715, 'std_time_ms': 3.3022165298461914, 'throughput_tokens_per_sec': 70891.0101591965, 'runs': 2, 'peak_memory_mb': 5.046875, 'theoretical_memory_mb': 2.125, 'memory_efficiency': 0.42105263157894735}}, 2048: {1: {'avg_time_ms': 5.719661712646484, 'std_time_ms': 1.0383129119873047, 'throughput_tokens_per_sec': 358063.1343059608, 'runs': 2, 'peak_memory_mb': 130.0, 'theoretical_memory_mb': 8.0, 'memory_efficiency': 0.06153846153846154}, 2: {'avg_time_ms': 208.1451416015625, 'std_time_ms': 70.48583030700684, 'throughput_tokens_per_sec': 9839.288028736895, 'runs': 2, 'peak_memory_mb': 98.125, 'theoretical_memory_mb': 6.0, 'memory_efficiency': 0.061146496815286625}, 4: {'avg_time_ms': 36.1483097076416, 'std_time_ms': 4.790782928466797, 'throughput_tokens_per_sec': 56655.48449052547, 'runs': 2, 'peak_memory_mb': 50.125, 'theoretical_memory_mb': 5.0, 'memory_efficiency': 0.09975062344139651}, 8: {'avg_time_ms': 35.300254821777344, 'std_time_ms': 10.788440704345703, 'throughput_tokens_per_sec': 58016.57836012427, 'runs': 2, 'peak_memory_mb': 26.125, 'theoretical_memory_mb': 4.5, 'memory_efficiency': 0.1722488038277512}, 16: {'avg_time_ms': 32.29784965515137, 'std_time_ms': 5.425691604614258, 'throughput_tokens_per_sec': 63409.79420818354, 'runs': 2, 'peak_memory_mb': 14.125, 'theoretical_memory_mb': 4.25, 'memory_efficiency': 0.3008849557522124}}, 4096: {1: {'avg_time_ms': 474.05779361724854, 'std_time_ms': 189.36192989349365, 'throughput_tokens_per_sec': 8640.296721515533, 'runs': 2, 'peak_memory_mb': 516.0, 'theoretical_memory_mb': 16.0, 'memory_efficiency': 0.031007751937984496}, 2: {'avg_time_ms': 115.95821380615234, 'std_time_ms': 20.356178283691406, 'throughput_tokens_per_sec': 35323.069108733376, 'runs': 2, 'peak_memory_mb': 388.25, 'theoretical_memory_mb': 12.0, 'memory_efficiency': 0.030907920154539602}, 4: {'avg_time_ms': 103.6900281906128, 'std_time_ms': 1.3593435287475586, 'throughput_tokens_per_sec': 39502.352072567155, 'runs': 2, 'peak_memory_mb': 196.25, 'theoretical_memory_mb': 10.0, 'memory_efficiency': 0.050955414012738856}, 8: {'avg_time_ms': 621.9077110290527, 'std_time_ms': 389.03141021728516, 'throughput_tokens_per_sec': 6586.186225641852, 'runs': 2, 'peak_memory_mb': 100.25, 'theoretical_memory_mb': 9.0, 'memory_efficiency': 0.08977556109725686}, 16: {'avg_time_ms': 650.0260829925537, 'std_time_ms': 72.05867767333984, 'throughput_tokens_per_sec': 6301.285605560725, 'runs': 2, 'peak_memory_mb': 52.25, 'theoretical_memory_mb': 8.5, 'memory_efficiency': 0.16267942583732056}}, 8192: {1: {'avg_time_ms': 260.1684331893921, 'std_time_ms': 95.10886669158936, 'throughput_tokens_per_sec': 31487.29421004183, 'runs': 2, 'peak_memory_mb': 2056.0, 'theoretical_memory_mb': 32.0, 'memory_efficiency': 0.01556420233463035}, 2: {'avg_time_ms': 894.3746089935303, 'std_time_ms': 441.8933391571045, 'throughput_tokens_per_sec': 9159.472907240437, 'runs': 2, 'peak_memory_mb': 1544.5, 'theoretical_memory_mb': 24.0, 'memory_efficiency': 0.015539009388151505}, 4: {'avg_time_ms': 1051.7163276672363, 'std_time_ms': 578.5431861877441, 'throughput_tokens_per_sec': 7789.172597681638, 'runs': 2, 'peak_memory_mb': 776.5, 'theoretical_memory_mb': 20.0, 'memory_efficiency': 0.025756600128783}, 8: {'avg_time_ms': 1125.6351470947266, 'std_time_ms': 617.774248123169, 'throughput_tokens_per_sec': 7277.668986388368, 'runs': 2, 'peak_memory_mb': 392.5, 'theoretical_memory_mb': 18.0, 'memory_efficiency': 0.045859872611464965}, 16: {'avg_time_ms': 1194.9771642684937, 'std_time_ms': 288.6101007461548, 'throughput_tokens_per_sec': 6855.361127352371, 'runs': 2, 'peak_memory_mb': 200.5, 'theoretical_memory_mb': 17.0, 'memory_efficiency': 0.08478802992518704}}}} | {'implementation': 'StandardAttention', 'device': 'cuda', 'dtype': 'torch.float16', 'seq_length_results': {1024: {1: {'avg_time_ms': 1.2601613998413086, 'std_time_ms': 0.00011920928955078125, 'throughput_tokens_per_sec': 812594.3233374326, 'runs': 2, 'peak_memory_mb': 32.0, 'theoretical_memory_mb': 131.0, 'memory_efficiency': 4.09375}}, 2048: {1: {'avg_time_ms': 5.424022674560547, 'std_time_ms': 0.09512901306152344, 'throughput_tokens_per_sec': 377579.5425054945, 'runs': 2, 'peak_memory_mb': 128.0, 'theoretical_memory_mb': 518.0, 'memory_efficiency': 4.046875}}, 4096: {1: {'avg_time_ms': 30.271530151367188, 'std_time_ms': 0.04839897155761719, 'throughput_tokens_per_sec': 135308.65402306092, 'runs': 2, 'peak_memory_mb': 512.0, 'theoretical_memory_mb': 2060.0, 'memory_efficiency': 4.0234375}}, 8192: {1: {'error': 'CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 7.88 GiB of which 805.69 MiB is free. Process 3093175 has 16.45 MiB memory in use. Including non-PyTorch memory, this process has 2.41 GiB memory in use. Of the allocated memory 2.04 GiB is allocated by PyTorch, and 255.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)'}}}} | N/A | N/A | N/A |
| summary | N/A | N/A | N/A | 89.79 | ['StandardAttention', 'RingAttentionCorrectV2', 'RingDilatedAttentionV2'] | 8192 |