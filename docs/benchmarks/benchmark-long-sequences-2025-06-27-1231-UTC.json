{
  "metadata": {
    "timestamp": "2025-06-27-1231-UTC",
    "device": "NVIDIA GeForce GTX 1080",
    "total_memory_gb": 7.8841552734375,
    "dtype": "float16",
    "num_heads": 8,
    "head_dim": 64
  },
  "results": {
    "ImprovedDilatedAttention": [
      {
        "implementation": "ImprovedDilatedAttention",
        "seq_len": 8192,
        "batch_size": 8,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": null,
        "mean_time_ms": 574.2680346593261,
        "std_time_ms": 70.10096883105892,
        "peak_memory_mb": 388.03125,
        "memory_per_token": 0.005920886993408203,
        "throughput_tokens_per_sec": 114120.92619586257,
        "success": true,
        "error": null
      },
      {
        "implementation": "ImprovedDilatedAttention",
        "seq_len": 16384,
        "batch_size": 8,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": null,
        "mean_time_ms": 1109.9352575838566,
        "std_time_ms": 12.709702324345836,
        "peak_memory_mb": 784.15625,
        "memory_per_token": 0.005982637405395508,
        "throughput_tokens_per_sec": 118089.77064601211,
        "success": true,
        "error": null
      },
      {
        "implementation": "ImprovedDilatedAttention",
        "seq_len": 32768,
        "batch_size": 4,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": null,
        "mean_time_ms": 1383.158419591685,
        "std_time_ms": 56.860218141196015,
        "peak_memory_mb": 696.1875,
        "memory_per_token": 0.005311489105224609,
        "throughput_tokens_per_sec": 94762.82553280707,
        "success": true,
        "error": null
      },
      {
        "implementation": "ImprovedDilatedAttention",
        "seq_len": 65536,
        "batch_size": 2,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": null,
        "mean_time_ms": 1332.391494885087,
        "std_time_ms": 0.6869132519627174,
        "peak_memory_mb": 696.25,
        "memory_per_token": 0.0053119659423828125,
        "throughput_tokens_per_sec": 98373.48895063638,
        "success": true,
        "error": null
      }
    ],
    "RingDilatedAttention": [
      {
        "implementation": "RingDilatedAttention",
        "seq_len": 8192,
        "batch_size": 8,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": 1,
        "mean_time_ms": 569.7534879048666,
        "std_time_ms": 40.69169897787036,
        "peak_memory_mb": 484.03125,
        "memory_per_token": 0.007385730743408203,
        "throughput_tokens_per_sec": 115025.18438455394,
        "success": true,
        "error": null
      },
      {
        "implementation": "RingDilatedAttention",
        "seq_len": 16384,
        "batch_size": 8,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": 2,
        "mean_time_ms": 1440.1104937617977,
        "std_time_ms": 255.77537012457518,
        "peak_memory_mb": 976.15625,
        "memory_per_token": 0.007447481155395508,
        "throughput_tokens_per_sec": 91015.23846105661,
        "success": true,
        "error": null
      },
      {
        "implementation": "RingDilatedAttention",
        "seq_len": 32768,
        "batch_size": 4,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": 1,
        "mean_time_ms": 1410.439127124846,
        "std_time_ms": 110.18459331169903,
        "peak_memory_mb": 824.1875,
        "memory_per_token": 0.006288051605224609,
        "throughput_tokens_per_sec": 92929.923368751,
        "success": true,
        "error": null
      },
      {
        "implementation": "RingDilatedAttention",
        "seq_len": 65536,
        "batch_size": 2,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": 1,
        "mean_time_ms": 1335.3097398454945,
        "std_time_ms": 21.988503001006038,
        "peak_memory_mb": 824.25,
        "memory_per_token": 0.0062885284423828125,
        "throughput_tokens_per_sec": 98158.49917725158,
        "success": true,
        "error": null
      }
    ],
    "BlockSparseRingDilatedAttention": [
      {
        "implementation": "BlockSparseRingDilatedAttention",
        "seq_len": 8192,
        "batch_size": 8,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": 1,
        "mean_time_ms": 364.11066850026447,
        "std_time_ms": 20.073284686867765,
        "peak_memory_mb": 273.1318359375,
        "memory_per_token": 0.004167661070823669,
        "throughput_tokens_per_sec": 179989.233136003,
        "success": true,
        "error": null
      },
      {
        "implementation": "BlockSparseRingDilatedAttention",
        "seq_len": 16384,
        "batch_size": 8,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": 1,
        "mean_time_ms": 1196.5934112668037,
        "std_time_ms": 429.72226141990376,
        "peak_memory_mb": 529.138671875,
        "memory_per_token": 0.004037007689476013,
        "throughput_tokens_per_sec": 109537.62469846573,
        "success": true,
        "error": null
      },
      {
        "implementation": "BlockSparseRingDilatedAttention",
        "seq_len": 32768,
        "batch_size": 4,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": 1,
        "mean_time_ms": 1200.3973905617993,
        "std_time_ms": 477.98636791749936,
        "peak_memory_mb": 524.65234375,
        "memory_per_token": 0.004002779722213745,
        "throughput_tokens_per_sec": 109190.5072691443,
        "success": true,
        "error": null
      },
      {
        "implementation": "BlockSparseRingDilatedAttention",
        "seq_len": 65536,
        "batch_size": 2,
        "num_heads": 8,
        "head_dim": 64,
        "ring_size": 1,
        "mean_time_ms": 1256.6778458033998,
        "std_time_ms": 16.732328166907894,
        "peak_memory_mb": 522.4296875,
        "memory_per_token": 0.0039858222007751465,
        "throughput_tokens_per_sec": 104300.39841770672,
        "success": true,
        "error": null
      }
    ]
  }
}