{
  "timestamp": "2025-06-28-0714-UTC",
  "device": "cuda",
  "gpu_type": "generic_cuda",
  "results": [
    {
      "implementation": "Baseline",
      "phase_features": [],
      "seq_len": 1024,
      "batch_size": 4,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 30.73535729199648,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 133266.7117250855,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.1 (Bug Fixes)",
      "phase_features": [
        "bug_fixes"
      ],
      "seq_len": 1024,
      "batch_size": 4,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 28.048978839069605,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 146030.27167230257,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": 1.0957745545155106,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.3 (FA3)",
      "phase_features": [
        "bug_fixes",
        "fa3"
      ],
      "seq_len": 1024,
      "batch_size": 4,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([4, 1024, 8, 64]), key=torch.Size([4, 1024, 8, 64]), value=torch.Size([4, 1024, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Phase 1.4 (Memory Pools)",
      "phase_features": [
        "bug_fixes",
        "memory_pools"
      ],
      "seq_len": 1024,
      "batch_size": 4,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 33.047569543123245,
      "memory_mb": 40.0,
      "throughput_tokens_sec": 123942.5487751889,
      "memory_per_token_kb": 10.0,
      "speedup_vs_baseline": 0.9300338184292313,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Full Phase 1",
      "phase_features": [
        "bug_fixes",
        "fa3",
        "memory_pools"
      ],
      "seq_len": 1024,
      "batch_size": 4,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([4, 1024, 8, 64]), key=torch.Size([4, 1024, 8, 64]), value=torch.Size([4, 1024, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Baseline",
      "phase_features": [],
      "seq_len": 4096,
      "batch_size": 2,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 298.3366581611335,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 27458.911856468705,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.1 (Bug Fixes)",
      "phase_features": [
        "bug_fixes"
      ],
      "seq_len": 4096,
      "batch_size": 2,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 139.53258777037263,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 58710.29937093613,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": 2.1381145646929673,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.3 (FA3)",
      "phase_features": [
        "bug_fixes",
        "fa3"
      ],
      "seq_len": 4096,
      "batch_size": 2,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([2, 4096, 8, 64]), key=torch.Size([2, 4096, 8, 64]), value=torch.Size([2, 4096, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Phase 1.4 (Memory Pools)",
      "phase_features": [
        "bug_fixes",
        "memory_pools"
      ],
      "seq_len": 4096,
      "batch_size": 2,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 67.64129418879747,
      "memory_mb": 144.0,
      "throughput_tokens_sec": 121109.45093887237,
      "memory_per_token_kb": 18.0,
      "speedup_vs_baseline": 4.410569929789177,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Full Phase 1",
      "phase_features": [
        "bug_fixes",
        "fa3",
        "memory_pools"
      ],
      "seq_len": 4096,
      "batch_size": 2,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([2, 4096, 8, 64]), key=torch.Size([2, 4096, 8, 64]), value=torch.Size([2, 4096, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Ring Attention + Phase 1",
      "phase_features": [
        "bug_fixes",
        "memory_pools",
        "ring"
      ],
      "seq_len": 4096,
      "batch_size": 2,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 62.21423353999853,
      "memory_mb": 124.0,
      "throughput_tokens_sec": 131674.04842708915,
      "memory_per_token_kb": 15.5,
      "speedup_vs_baseline": 4.795311959751591,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Block Sparse + Phase 1",
      "phase_features": [
        "bug_fixes",
        "memory_pools",
        "block_sparse"
      ],
      "seq_len": 4096,
      "batch_size": 2,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 53.11248544603586,
      "memory_mb": 160.0,
      "throughput_tokens_sec": 154238.68665162276,
      "memory_per_token_kb": 20.0,
      "speedup_vs_baseline": 5.617072062354415,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Baseline",
      "phase_features": [],
      "seq_len": 8192,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 333.464721404016,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 24566.31683708099,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.1 (Bug Fixes)",
      "phase_features": [
        "bug_fixes"
      ],
      "seq_len": 8192,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 148.48244953900576,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 55171.50360486203,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": 2.245819101444822,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.3 (FA3)",
      "phase_features": [
        "bug_fixes",
        "fa3"
      ],
      "seq_len": 8192,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([1, 8192, 8, 64]), key=torch.Size([1, 8192, 8, 64]), value=torch.Size([1, 8192, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Phase 1.4 (Memory Pools)",
      "phase_features": [
        "bug_fixes",
        "memory_pools"
      ],
      "seq_len": 8192,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 78.32244886085391,
      "memory_mb": 144.0,
      "throughput_tokens_sec": 104593.25671179847,
      "memory_per_token_kb": 18.0,
      "speedup_vs_baseline": 4.257588038346998,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Full Phase 1",
      "phase_features": [
        "bug_fixes",
        "fa3",
        "memory_pools"
      ],
      "seq_len": 8192,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([1, 8192, 8, 64]), key=torch.Size([1, 8192, 8, 64]), value=torch.Size([1, 8192, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Ring Attention + Phase 1",
      "phase_features": [
        "bug_fixes",
        "memory_pools",
        "ring"
      ],
      "seq_len": 8192,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 149.91481713950634,
      "memory_mb": 88.0,
      "throughput_tokens_sec": 54644.365088854196,
      "memory_per_token_kb": 11.0,
      "speedup_vs_baseline": 2.2243613257634403,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Block Sparse + Phase 1",
      "phase_features": [
        "bug_fixes",
        "memory_pools",
        "block_sparse"
      ],
      "seq_len": 8192,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 90.6487145461142,
      "memory_mb": 160.0,
      "throughput_tokens_sec": 90370.83472190464,
      "memory_per_token_kb": 20.0,
      "speedup_vs_baseline": 3.678648098582557,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Baseline",
      "phase_features": [],
      "seq_len": 16384,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 578.7318418733776,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 28310.17548121139,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.1 (Bug Fixes)",
      "phase_features": [
        "bug_fixes"
      ],
      "seq_len": 16384,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 253.79781303927302,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 64555.32379810033,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": 2.280286953393975,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.3 (FA3)",
      "phase_features": [
        "bug_fixes",
        "fa3"
      ],
      "seq_len": 16384,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([1, 16384, 8, 64]), key=torch.Size([1, 16384, 8, 64]), value=torch.Size([1, 16384, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Phase 1.4 (Memory Pools)",
      "phase_features": [
        "bug_fixes",
        "memory_pools"
      ],
      "seq_len": 16384,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 314.19631727039814,
      "memory_mb": 78.0,
      "throughput_tokens_sec": 52145.74168894503,
      "memory_per_token_kb": 4.875,
      "speedup_vs_baseline": 1.8419434285581378,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Full Phase 1",
      "phase_features": [
        "bug_fixes",
        "fa3",
        "memory_pools"
      ],
      "seq_len": 16384,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([1, 16384, 8, 64]), key=torch.Size([1, 16384, 8, 64]), value=torch.Size([1, 16384, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Ring Attention + Phase 1",
      "phase_features": [
        "bug_fixes",
        "memory_pools",
        "ring"
      ],
      "seq_len": 16384,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 584.2032261192799,
      "memory_mb": 152.0,
      "throughput_tokens_sec": 28045.035130728276,
      "memory_per_token_kb": 9.5,
      "speedup_vs_baseline": 0.990634450476682,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Block Sparse + Phase 1",
      "phase_features": [
        "bug_fixes",
        "memory_pools",
        "block_sparse"
      ],
      "seq_len": 16384,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 232.3581552132964,
      "memory_mb": 160.0,
      "throughput_tokens_sec": 70511.83542475657,
      "memory_per_token_kb": 10.0,
      "speedup_vs_baseline": 2.4906887444605617,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Baseline",
      "phase_features": [],
      "seq_len": 32768,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 1703.424084559083,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 19236.548489028628,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.1 (Bug Fixes)",
      "phase_features": [
        "bug_fixes"
      ],
      "seq_len": 32768,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 627.592465095222,
      "memory_mb": 0.0,
      "throughput_tokens_sec": 52212.22659999311,
      "memory_per_token_kb": 0.0,
      "speedup_vs_baseline": 2.7142201018946737,
      "memory_reduction_vs_baseline": 0,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": null,
      "notes": null
    },
    {
      "implementation": "Phase 1.3 (FA3)",
      "phase_features": [
        "bug_fixes",
        "fa3"
      ],
      "seq_len": 32768,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([1, 32768, 8, 64]), key=torch.Size([1, 32768, 8, 64]), value=torch.Size([1, 32768, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Phase 1.4 (Memory Pools)",
      "phase_features": [
        "bug_fixes",
        "memory_pools"
      ],
      "seq_len": 32768,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 7.88 GiB of which 128.81 MiB is free. Process 3093175 has 16.45 MiB memory in use. Including non-PyTorch memory, this process has 2.33 GiB memory in use. Of the allocated memory 2.18 GiB is allocated by PyTorch, and 24.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "notes": null
    },
    {
      "implementation": "Full Phase 1",
      "phase_features": [
        "bug_fixes",
        "fa3",
        "memory_pools"
      ],
      "seq_len": 32768,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([1, 32768, 8, 64]), key=torch.Size([1, 32768, 8, 64]), value=torch.Size([1, 32768, 8, 64])",
      "notes": null
    },
    {
      "implementation": "Ring Attention + Phase 1",
      "phase_features": [
        "bug_fixes",
        "memory_pools",
        "ring"
      ],
      "seq_len": 32768,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.88 GiB of which 140.81 MiB is free. Process 3093175 has 16.45 MiB memory in use. Including non-PyTorch memory, this process has 2.32 GiB memory in use. Of the allocated memory 2.15 GiB is allocated by PyTorch, and 48.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "notes": null
    },
    {
      "implementation": "Block Sparse + Phase 1",
      "phase_features": [
        "bug_fixes",
        "memory_pools",
        "block_sparse"
      ],
      "seq_len": 32768,
      "batch_size": 1,
      "num_heads": 8,
      "head_dim": 64,
      "time_ms": 0,
      "memory_mb": 0,
      "throughput_tokens_sec": 0,
      "memory_per_token_kb": 0,
      "speedup_vs_baseline": null,
      "memory_reduction_vs_baseline": null,
      "gpu_type": "generic_cuda",
      "has_flash_attn": false,
      "has_flash_attn_3": false,
      "error": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.88 GiB of which 140.75 MiB is free. Process 3093175 has 16.45 MiB memory in use. Including non-PyTorch memory, this process has 2.32 GiB memory in use. Of the allocated memory 2.15 GiB is allocated by PyTorch, and 48.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "notes": null
    }
  ]
}