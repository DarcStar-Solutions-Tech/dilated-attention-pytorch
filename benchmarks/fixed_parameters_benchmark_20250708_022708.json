{
  "timestamp": "2025-07-08T02:27:08.868052+00:00",
  "device": "cuda",
  "cuda_available": true,
  "note": "Fixed parameter benchmark - addresses all known parameter issues",
  "results": [
    {
      "implementation": "DilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.2767772674560547,
      "backward_time_ms": 36.05625629425049,
      "total_time_ms": 39.333033561706536,
      "peak_memory_mb": 182.0947265625,
      "throughput_tokens_per_sec": 1250008.671837483,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedDilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.3512353897094727,
      "backward_time_ms": 18.600153923034668,
      "total_time_ms": 21.95138931274414,
      "peak_memory_mb": 180.5009765625,
      "throughput_tokens_per_sec": 1222235.8395287455,
      "success": true,
      "error": null
    },
    {
      "implementation": "RingDilatedAttentionProduction",
      "category": "ring",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "RingAttentionConfig.__init__() got an unexpected keyword argument 'block_size'"
    },
    {
      "implementation": "RingDilatedAttentionProductionFixed",
      "category": "ring",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 9.267330169677734,
      "backward_time_ms": 215.3430461883545,
      "total_time_ms": 224.61037635803223,
      "peak_memory_mb": 1438.2744140625,
      "throughput_tokens_per_sec": 441982.7420632879,
      "success": true,
      "error": null
    },
    {
      "implementation": "HilbertAttentionTritonFixed",
      "category": "kernels",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "HilbertAttentionTritonFixed.forward() takes from 2 to 3 positional arguments but 4 were given"
    },
    {
      "implementation": "MultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 25.69408416748047,
      "backward_time_ms": 117.68214702606201,
      "total_time_ms": 143.37623119354248,
      "peak_memory_mb": 243.50048828125,
      "throughput_tokens_per_sec": 159414.12713141466,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedMultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 39.77689743041992,
      "backward_time_ms": 188.27691078186035,
      "total_time_ms": 228.05380821228027,
      "peak_memory_mb": 292.19775390625,
      "throughput_tokens_per_sec": 102974.34603000304,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttention",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 51.9343376159668,
      "backward_time_ms": 537.8068685531616,
      "total_time_ms": 589.7412061691284,
      "peak_memory_mb": 518.25390625,
      "throughput_tokens_per_sec": 78868.82143926136,
      "success": true,
      "error": null
    }
  ]
}