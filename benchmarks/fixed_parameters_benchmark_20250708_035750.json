{
  "timestamp": "2025-07-08T03:57:50.324951+00:00",
  "device": "cuda",
  "cuda_available": true,
  "note": "Fixed parameter benchmark - addresses all known parameter issues",
  "results": [
    {
      "implementation": "DilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.846120834350586,
      "backward_time_ms": 37.28013038635254,
      "total_time_ms": 41.126251220703125,
      "peak_memory_mb": 182.0947265625,
      "throughput_tokens_per_sec": 1064969.1407034553,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedDilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 4.005312919616699,
      "backward_time_ms": 23.48024845123291,
      "total_time_ms": 27.48556137084961,
      "peak_memory_mb": 180.5009765625,
      "throughput_tokens_per_sec": 1022641.6967171642,
      "success": true,
      "error": null
    },
    {
      "implementation": "HilbertAttentionTritonFixed",
      "category": "kernels",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "mat1 and mat2 shapes cannot be multiplied (4096x768 and 512x1536)"
    },
    {
      "implementation": "MultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 79.28006649017334,
      "backward_time_ms": 65.68982601165771,
      "total_time_ms": 144.96989250183105,
      "peak_memory_mb": 244.12548828125,
      "throughput_tokens_per_sec": 51664.94153366652,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedMultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 71.8407392501831,
      "backward_time_ms": 250.06248950958255,
      "total_time_ms": 321.9032287597656,
      "peak_memory_mb": 294.32275390625,
      "throughput_tokens_per_sec": 57015.00350289839,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttention",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 16.24774932861328,
      "backward_time_ms": 835.492730140686,
      "total_time_ms": 851.7404794692993,
      "peak_memory_mb": 518.25390625,
      "throughput_tokens_per_sec": 252096.45454011858,
      "success": true,
      "error": null
    }
  ]
}