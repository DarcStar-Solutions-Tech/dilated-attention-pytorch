{
  "timestamp": "2025-07-08 03:49 UTC",
  "device": "cuda",
  "test_config": {
    "batch_size": 2,
    "seq_len": 2048,
    "num_heads": 8,
    "head_dim": 64
  },
  "implementations": {
    "DilatedAttention": {
      "status": "failed",
      "error": "dilated_attention_pytorch.core.config.DilatedAttentionConfig() got multiple values for keyword argument 'dropout'"
    },
    "ImprovedDilatedAttention": {
      "status": "working",
      "forward_time_ms": 4.0,
      "memory_mb": 40
    },
    "MultiheadDilatedAttention": {
      "status": "failed",
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([2, 2048, 8, 64]), key=torch.Size([2, 2048, 8, 64]), value=torch.Size([2, 2048, 8, 64])"
    },
    "ImprovedMultiheadDilatedAttention": {
      "status": "failed",
      "error": "Expected 3D tensors (batch, seq_len, embed_dim), got shapes: query=torch.Size([2, 2048, 8, 64]), key=torch.Size([2, 2048, 8, 64]), value=torch.Size([2, 2048, 8, 64])"
    },
    "BlockSparseRingDilatedAttention": {
      "status": "working",
      "forward_time_ms": 14.1,
      "memory_mb": 48
    },
    "BlockSparseRingMultiheadDilatedAttention": {
      "status": "failed",
      "error": "too many values to unpack (expected 3)"
    },
    "BlockSparseAdaptive": {
      "status": "failed",
      "error": "BlockSparseAdaptive.__init__() missing 2 required positional arguments: 'num_heads' and 'head_dim'"
    }
  }
}