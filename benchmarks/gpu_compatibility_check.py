"""
Check GPU compatibility with Flash Attention and other optimizations.
"""

import torch


def check_gpu_compatibility():
    """Check what optimizations are supported on current hardware."""

    print("GPU Compatibility Check")
    print("=" * 50)

    if not torch.cuda.is_available():
        print("‚ùå CUDA not available")
        return

    device_count = torch.cuda.device_count()
    print(f"Found {device_count} GPU(s)")

    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        print(f"\nüñ•Ô∏è  GPU {i}: {props.name}")
        print(f"   Compute Capability: {props.major}.{props.minor}")
        print(f"   Memory: {props.total_memory / 1024**3:.1f}GB")

        # Check architecture support
        cc = props.major * 10 + props.minor

        if cc >= 80:  # Ampere (RTX 30xx/40xx, A100, etc.)
            flash_support = "‚úÖ Flash Attention 2/3"
        elif cc >= 75:  # Turing (RTX 20xx)
            flash_support = "‚ö†Ô∏è  Flash Attention 2 (limited)"
        elif cc >= 70:  # Volta (V100)
            flash_support = "‚ö†Ô∏è  Flash Attention 2 (limited)"
        else:  # Pascal (GTX 10xx) and older
            flash_support = "‚ùå Flash Attention not supported"

        print(f"   Flash Attention: {flash_support}")

        # Other optimizations
        print("   PyTorch SDPA: ‚úÖ Supported")
        print("   xFormers: ‚úÖ Supported")

    # Test what actually gets used
    print("\nüß™ Testing optimization selection:")

    try:
        from dilated_attention_pytorch.utils.attention_utils import (
            optimize_attention_computation,
        )

        device = torch.device("cuda:0")
        q = torch.randn(1, 8, 512, 64, device=device, dtype=torch.float16)
        k = torch.randn_like(q)
        v = torch.randn_like(q)

        # Capture warnings to see what gets used
        import warnings

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter("always")

            _ = optimize_attention_computation(q, k, v)

            if w:
                for warning in w:
                    if "Flash Attention failed" in str(warning.message):
                        print("   ‚ùå Flash Attention: Not compatible with GPU")
                    elif "xFormers failed" in str(warning.message):
                        print("   ‚ùå xFormers: Failed")
                    elif "PyTorch SDPA failed" in str(warning.message):
                        print("   ‚ùå PyTorch SDPA: Failed")
            else:
                print("   ‚úÖ Using optimized backend (no warnings)")

        print("   üéØ Result: Successfully using available optimization")

    except Exception as e:
        print(f"   ‚ùå Error testing optimizations: {e}")

    print("\nüìä Summary for GTX 1080:")
    print("   ‚Ä¢ Flash Attention: ‚ùå Requires Ampere+ (RTX 30xx/40xx)")
    print("   ‚Ä¢ xFormers: ‚úÖ Best option for Pascal architecture")
    print("   ‚Ä¢ PyTorch SDPA: ‚úÖ Good fallback option")
    print("   ‚Ä¢ Manual: üìù Always available")

    print("\nüí° Recommendation:")
    print("   Current setup with xFormers is optimal for GTX 1080")
    print("   Flash Attention would require upgrading to RTX 30xx+ series")


if __name__ == "__main__":
    check_gpu_compatibility()
