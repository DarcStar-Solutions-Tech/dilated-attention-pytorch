"""
Test different Flash Attention versions for Pascal GPU compatibility.
"""

import torch
import subprocess
import sys


def test_flash_attention_version(version=None):
    """Test if a specific Flash Attention version works on Pascal GPU."""

    try:
        import flash_attn

        current_version = flash_attn.__version__

        print(f"Testing Flash Attention v{current_version}")

        # Test basic Flash Attention functionality
        device = torch.device("cuda:0")

        # Small test tensors
        batch_size, num_heads, seq_len, head_dim = 1, 4, 256, 32

        q = torch.randn(
            batch_size, seq_len, num_heads, head_dim, device=device, dtype=torch.float16
        )
        k = torch.randn_like(q)
        v = torch.randn_like(q)

        print(f"Input shapes: q={q.shape}, k={k.shape}, v={v.shape}")
        print(f"GPU: {torch.cuda.get_device_properties(0).name}")
        print(
            f"Compute capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}"
        )

        # Try Flash Attention functions
        try:
            from flash_attn import flash_attn_func

            print("‚úÖ flash_attn_func import successful")

            # Test with minimal parameters
            output = flash_attn_func(
                q, k, v, dropout_p=0.0, softmax_scale=None, causal=False
            )
            print(
                f"‚úÖ flash_attn_func execution successful! Output shape: {output.shape}"
            )
            return True

        except Exception as e:
            print(f"‚ùå flash_attn_func failed: {e}")

        try:
            from flash_attn.flash_attn_interface import (
                flash_attn_func as flash_attn_interface,
            )

            print("‚úÖ flash_attn_interface import successful")

            output = flash_attn_interface(
                q, k, v, dropout_p=0.0, softmax_scale=None, causal=False
            )
            print(
                f"‚úÖ flash_attn_interface execution successful! Output shape: {output.shape}"
            )
            return True

        except Exception as e:
            print(f"‚ùå flash_attn_interface failed: {e}")

        return False

    except ImportError as e:
        print(f"‚ùå Flash Attention import failed: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        return False


def check_available_versions():
    """Check what Flash Attention versions are available."""

    print("Available Flash Attention Versions")
    print("=" * 50)

    try:
        # Try to get version info from pip
        result = subprocess.run(
            [sys.executable, "-m", "pip", "index", "versions", "flash-attn"],
            capture_output=True,
            text=True,
        )
        if result.returncode == 0:
            print(result.stdout)
        else:
            print("Could not retrieve version list via pip")
    except Exception:
        print("pip index command not available")

    # Known early versions to try
    early_versions = [
        "0.2.8",  # Early version
        "1.0.0",  # First major release
        "1.0.8",  # Stable 1.x
        "2.0.0",  # First 2.x
        "2.1.0",  # Early 2.x
    ]

    print("\nKnown early versions that might support Pascal:")
    for version in early_versions:
        print(f"  ‚Ä¢ flash-attn=={version}")

    return early_versions


def main():
    """Test Flash Attention compatibility."""

    print("Flash Attention Pascal GPU Compatibility Test")
    print("=" * 60)

    # Test current version
    print("\nüß™ Testing current installation:")
    current_works = test_flash_attention_version()

    if current_works:
        print("\nüéâ Current Flash Attention version works on Pascal GPU!")
        return

    print("\nüìã Analysis of Pascal GPU Support:")
    print("Flash Attention was designed for Ampere+ GPUs (RTX 30xx/40xx)")
    print("Pascal GPUs (GTX 10xx) have compute capability 6.1")
    print("Flash Attention requires compute capability 7.0+ (Volta) or 8.0+ (Ampere)")

    print("\nüí° Recommendations:")
    print("1. ‚úÖ Current setup with xFormers is optimal for GTX 1080")
    print("2. ‚úÖ xFormers provides substantial performance gains")
    print("3. ‚úÖ PyTorch SDPA also works well as fallback")
    print("4. üîÑ For Flash Attention, hardware upgrade to RTX 30xx+ needed")

    print("\nüìä Performance Comparison (from earlier tests):")
    print("‚Ä¢ Manual attention: ~133ms (baseline)")
    print("‚Ä¢ xFormers optimized: ~17ms (7.6x speedup)")
    print("‚Ä¢ Flash Attention would be: ~10-15ms (est. 1.5-2x faster than xFormers)")

    print("\nüèÜ Conclusion:")
    print("xFormers provides excellent optimization for Pascal GPUs")
    print("Flash Attention upgrade requires hardware upgrade")


if __name__ == "__main__":
    main()
