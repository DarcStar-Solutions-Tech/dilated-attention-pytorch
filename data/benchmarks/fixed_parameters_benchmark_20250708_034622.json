{
  "timestamp": "2025-07-08T03:46:22.065802+00:00",
  "device": "cuda",
  "cuda_available": true,
  "note": "Fixed parameter benchmark - addresses all known parameter issues",
  "results": [
    {
      "implementation": "DilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 4.033613204956055,
      "backward_time_ms": 40.38829803466797,
      "total_time_ms": 44.42191123962402,
      "peak_memory_mb": 182.0947265625,
      "throughput_tokens_per_sec": 1015466.7271932001,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedDilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.8019895553588867,
      "backward_time_ms": 20.8848237991333,
      "total_time_ms": 24.686813354492188,
      "peak_memory_mb": 180.5009765625,
      "throughput_tokens_per_sec": 1077330.681833859,
      "success": true,
      "error": null
    },
    {
      "implementation": "HilbertAttentionTritonFixed",
      "category": "kernels",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "HilbertAttentionTritonFixed.forward() takes from 2 to 3 positional arguments but 4 were given"
    },
    {
      "implementation": "MultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 20.828557014465332,
      "backward_time_ms": 64.44666385650635,
      "total_time_ms": 85.27522087097168,
      "peak_memory_mb": 244.12548828125,
      "throughput_tokens_per_sec": 196653.08533641326,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedMultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 121.48830890655518,
      "backward_time_ms": 269.10688877105713,
      "total_time_ms": 390.5951976776123,
      "peak_memory_mb": 294.32275390625,
      "throughput_tokens_per_sec": 33715.17833169041,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttention",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 16.535210609436035,
      "backward_time_ms": 727.6362180709839,
      "total_time_ms": 744.1714286804199,
      "peak_memory_mb": 518.25390625,
      "throughput_tokens_per_sec": 247713.80883788466,
      "success": true,
      "error": null
    }
  ]
}