{
  "timestamp": "2025-07-08T00:45:57.510526",
  "device": "cuda",
  "results": [
    {
      "implementation": "DilatedAttention",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "dilated_attention_pytorch.core.config.DilatedAttentionConfig() got multiple values for keyword argument 'dropout'",
      "notes": null
    },
    {
      "implementation": "MultiheadDilatedAttention",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 8.405041694641113,
      "backward_time_ms": 51.43311023712158,
      "total_time_ms": 59.8381519317627,
      "peak_memory_mb": 244.12548828125,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "ImprovedDilatedAttention",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.2183170318603516,
      "backward_time_ms": 33.90839099884033,
      "total_time_ms": 37.126708030700684,
      "peak_memory_mb": 197.7509765625,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "ImprovedMultiheadDilatedAttention",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 46.178650856018066,
      "backward_time_ms": 136.41457557678223,
      "total_time_ms": 182.5932264328003,
      "peak_memory_mb": 294.32275390625,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "RingDilatedAttentionProduction",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "RingDilatedAttentionProduction.__init__() got an unexpected keyword argument 'segment_lengths'",
      "notes": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttention",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 14.643096923828125,
      "backward_time_ms": 413.5461091995239,
      "total_time_ms": 428.1892061233521,
      "peak_memory_mb": 518.25390625,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttentionFixed",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 16.055941581726074,
      "backward_time_ms": 577.6106834411621,
      "total_time_ms": 593.6666250228882,
      "peak_memory_mb": 518.25390625,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "BlockSparseRingMultiheadDilatedAttention",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 28.125309944152832,
      "backward_time_ms": 749.6815919876099,
      "total_time_ms": 777.8069019317627,
      "peak_memory_mb": 536.90234375,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "BlockSparseAdaptive",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "BlockSparseAdaptive.__init__() missing 2 required positional arguments: 'num_heads' and 'head_dim'",
      "notes": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttentionHilbertPostPattern",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "BlockSparseRingDilatedAttentionHilbertPostPattern.__init__() missing 1 required positional argument: 'sparse_config'",
      "notes": null
    },
    {
      "implementation": "HilbertDilatedAttention",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "HilbertDilatedAttention.__init__() got an unexpected keyword argument 'head_dim'",
      "notes": null
    },
    {
      "implementation": "DilatedAttention",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "dilated_attention_pytorch.core.config.DilatedAttentionConfig() got multiple values for keyword argument 'dropout'",
      "notes": null
    },
    {
      "implementation": "MultiheadDilatedAttention",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 149.09210205078125,
      "backward_time_ms": 249.6697187423706,
      "total_time_ms": 398.7618207931518,
      "peak_memory_mb": 452.32861328125,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "ImprovedDilatedAttention",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 70.88937759399414,
      "backward_time_ms": 50.81644058227539,
      "total_time_ms": 121.70581817626952,
      "peak_memory_mb": 378.2509765625,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "ImprovedMultiheadDilatedAttention",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 244.47040557861328,
      "backward_time_ms": 487.28504180908203,
      "total_time_ms": 731.7554473876953,
      "peak_memory_mb": 549.97900390625,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "RingDilatedAttentionProduction",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "RingDilatedAttentionProduction.__init__() got an unexpected keyword argument 'segment_lengths'",
      "notes": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttention",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 36.91394329071045,
      "backward_time_ms": 2999.1968870162964,
      "total_time_ms": 3036.110830307007,
      "peak_memory_mb": 1034.255859375,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttentionFixed",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 99.39439296722412,
      "backward_time_ms": 3483.8546752929688,
      "total_time_ms": 3583.249068260193,
      "peak_memory_mb": 1034.255859375,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "BlockSparseRingMultiheadDilatedAttention",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 339.3439054489136,
      "backward_time_ms": 4536.701560020447,
      "total_time_ms": 4876.04546546936,
      "peak_memory_mb": 1052.904296875,
      "success": true,
      "error": null,
      "notes": null
    },
    {
      "implementation": "BlockSparseAdaptive",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "BlockSparseAdaptive.__init__() missing 2 required positional arguments: 'num_heads' and 'head_dim'",
      "notes": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttentionHilbertPostPattern",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "BlockSparseRingDilatedAttentionHilbertPostPattern.__init__() missing 1 required positional argument: 'sparse_config'",
      "notes": null
    },
    {
      "implementation": "HilbertDilatedAttention",
      "batch_size": 2,
      "seq_len": 4096,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "success": false,
      "error": "HilbertDilatedAttention.__init__() got an unexpected keyword argument 'head_dim'",
      "notes": null
    }
  ]
}