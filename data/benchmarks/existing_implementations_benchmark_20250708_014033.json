{
  "timestamp": "2025-07-08T01:40:33.516766+00:00",
  "device": "cuda",
  "cuda_available": true,
  "results": [
    {
      "implementation": "DilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.5352706909179688,
      "backward_time_ms": 35.14673709869385,
      "total_time_ms": 38.682007789611816,
      "peak_memory_mb": 182.0947265625,
      "throughput_tokens_per_sec": 1158610.0070137577,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedDilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.2811641693115234,
      "backward_time_ms": 17.97618865966797,
      "total_time_ms": 21.257352828979492,
      "peak_memory_mb": 180.5009765625,
      "throughput_tokens_per_sec": 1248337.415820145,
      "success": true,
      "error": null
    },
    {
      "implementation": "MultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 55.16941547393799,
      "backward_time_ms": 49.56169128417969,
      "total_time_ms": 104.73110675811768,
      "peak_memory_mb": 244.12548828125,
      "throughput_tokens_per_sec": 74244.03475753606,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedMultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 107.56971836090088,
      "backward_time_ms": 233.16149711608887,
      "total_time_ms": 340.7312154769897,
      "peak_memory_mb": 294.32275390625,
      "throughput_tokens_per_sec": 38077.63060471861,
      "success": true,
      "error": null
    },
    {
      "implementation": "RingDilatedAttentionProduction",
      "category": "ring",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "RingAttentionConfig.__init__() got an unexpected keyword argument 'dim'"
    },
    {
      "implementation": "RingDilatedAttentionProductionFixed",
      "category": "ring",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
    },
    {
      "implementation": "RingDilatedAttentionHilbertOptimizedFixed",
      "category": "ring",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 62.1126651763916,
      "backward_time_ms": 88.45274448394775,
      "total_time_ms": 150.56540966033936,
      "peak_memory_mb": 396.1376953125,
      "throughput_tokens_per_sec": 65944.68275299268,
      "success": true,
      "error": null
    },
    {
      "implementation": "RingDistributedDilatedAttention",
      "category": "ring",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "Requires multi-GPU setup"
    },
    {
      "implementation": "BlockSparseRingDilatedAttention",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 14.522099494934082,
      "backward_time_ms": 536.3924264907837,
      "total_time_ms": 550.9145259857178,
      "peak_memory_mb": 518.25390625,
      "throughput_tokens_per_sec": 282052.88095077826,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttentionFixed",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 36.2886905670166,
      "backward_time_ms": 661.5357875823975,
      "total_time_ms": 697.8244781494141,
      "peak_memory_mb": 518.25390625,
      "throughput_tokens_per_sec": 112872.6315554335,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseRingMultiheadDilatedAttention",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 25.649070739746094,
      "backward_time_ms": 996.9000101089478,
      "total_time_ms": 1022.5490808486938,
      "peak_memory_mb": 536.90234375,
      "throughput_tokens_per_sec": 159693.89462725414,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseAdaptive",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 240.80770015716553,
      "backward_time_ms": 1525.6000995635986,
      "total_time_ms": 1766.4077997207642,
      "peak_memory_mb": 413.99560546875,
      "throughput_tokens_per_sec": 17009.42286034336,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttentionHilbertPostPattern",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 15.559816360473633,
      "backward_time_ms": 899.917483329773,
      "total_time_ms": 915.4772996902466,
      "peak_memory_mb": 518.2568359375,
      "throughput_tokens_per_sec": 263242.1813412276,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseRingDistributedDilatedAttention",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "Requires multi-GPU setup"
    },
    {
      "implementation": "HilbertDilatedAttention",
      "category": "kernels",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "CUDA compilation errors"
    },
    {
      "implementation": "HilbertAttentionTritonFixed",
      "category": "kernels",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "HilbertAttentionTritonFixed.__init__() got an unexpected keyword argument 'segment_lengths'"
    }
  ]
}