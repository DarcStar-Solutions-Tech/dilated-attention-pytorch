{
  "timestamp": "2025-07-08T03:59:18.596102+00:00",
  "device": "cuda",
  "cuda_available": true,
  "note": "Fixed parameter benchmark - addresses all known parameter issues",
  "results": [
    {
      "implementation": "DilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.433847427368164,
      "backward_time_ms": 37.22410202026367,
      "total_time_ms": 40.65794944763184,
      "peak_memory_mb": 182.0947265625,
      "throughput_tokens_per_sec": 1192831.098829378,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedDilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.8127899169921875,
      "backward_time_ms": 29.914498329162598,
      "total_time_ms": 33.727288246154785,
      "peak_memory_mb": 180.5009765625,
      "throughput_tokens_per_sec": 1074278.9634817408,
      "success": true,
      "error": null
    },
    {
      "implementation": "HilbertAttentionTritonFixed",
      "category": "kernels",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "shape '[2, 2048, 12, 64]' is invalid for input of size 2097152"
    },
    {
      "implementation": "MultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 11.99634075164795,
      "backward_time_ms": 64.38815593719482,
      "total_time_ms": 76.38449668884277,
      "peak_memory_mb": 243.50048828125,
      "throughput_tokens_per_sec": 341437.4503689659,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedMultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 74.06759262084961,
      "backward_time_ms": 378.7071228027344,
      "total_time_ms": 452.774715423584,
      "peak_memory_mb": 292.19775390625,
      "throughput_tokens_per_sec": 55300.83880229961,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttention",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 17.485785484313965,
      "backward_time_ms": 962.6084566116333,
      "total_time_ms": 980.0942420959473,
      "peak_memory_mb": 518.25390625,
      "throughput_tokens_per_sec": 234247.41220086528,
      "success": true,
      "error": null
    }
  ]
}