{
  "timestamp": "2025-07-08T03:45:16.628988+00:00",
  "device": "cuda",
  "cuda_available": true,
  "note": "Fixed parameter benchmark - addresses all known parameter issues",
  "results": [
    {
      "implementation": "DilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 4.025745391845703,
      "backward_time_ms": 41.58971309661865,
      "total_time_ms": 45.615458488464355,
      "peak_memory_mb": 182.0947265625,
      "throughput_tokens_per_sec": 1017451.3292113803,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedDilatedAttention",
      "category": "core",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 3.920602798461914,
      "backward_time_ms": 20.39499282836914,
      "total_time_ms": 24.315595626831055,
      "peak_memory_mb": 180.5009765625,
      "throughput_tokens_per_sec": 1044737.3045815545,
      "success": true,
      "error": null
    },
    {
      "implementation": "HilbertAttentionTritonFixed",
      "category": "kernels",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "HilbertAttentionTritonFixed.forward() takes from 2 to 3 positional arguments but 4 were given"
    },
    {
      "implementation": "MultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 43.13809871673584,
      "backward_time_ms": 59.479546546936035,
      "total_time_ms": 102.61764526367188,
      "peak_memory_mb": 244.12548828125,
      "throughput_tokens_per_sec": 94950.86992350263,
      "success": true,
      "error": null
    },
    {
      "implementation": "ImprovedMultiheadDilatedAttention",
      "category": "multihead",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 127.87699699401855,
      "backward_time_ms": 171.6625690460205,
      "total_time_ms": 299.53956604003906,
      "peak_memory_mb": 294.32275390625,
      "throughput_tokens_per_sec": 32030.780330191756,
      "success": true,
      "error": null
    },
    {
      "implementation": "BlockSparseRingDilatedAttention",
      "category": "block_sparse",
      "batch_size": 2,
      "seq_len": 2048,
      "num_heads": 12,
      "head_dim": 64,
      "forward_time_ms": 0.0,
      "backward_time_ms": 0.0,
      "total_time_ms": 0.0,
      "peak_memory_mb": 0.0,
      "throughput_tokens_per_sec": 0.0,
      "success": false,
      "error": "RingAttentionConfig.__init__() got an unexpected keyword argument 'use_gradient_checkpointing'"
    }
  ]
}